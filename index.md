

https://how2electronics.com/gesture-recognition-application-machine-learning/


# Gesture Recognition 

Recently, strong efforts have been carried out to develop intelligent and natural interfaces between users and computer-based systems based on human gestures. Gestures provide an intuitive interface to both humans and computers. Thus, such gesture-based interfaces can not only substitute the common interface devices but can also be exploited to extend their functionality. A robot is usually an electro-mechanical machine that can perform tasks automatically. Some robots require some degree of guidance, which may be done using a remote control or with a computer interface. Robots have evolved so much and are capable of mimicking humans that they seem to have a mind of their own. An important aspect of a successful robotic system is Human-Machine interaction. In the early years, the only way to communicate with a robot was to program which required extensive hard work. With the development in science and robotics, Gesture Recognition came into life.

## What is Gesture & Gesture Recognition?
Gestures originate from any bodily motion or state but commonly originate from the face or hand. Gesture Recognition can be considered as a way for a computer to understand human body language. This has minimized the need for text interfaces and GUIs (Graphical User Interface). A gesture is an action that has to be seen by someone else and has to convey some piece of information. The gesture is usually considered as a movement of part of the body, esp. a hand or the head, to express an idea or meaning. Gesture recognition technologies are much younger in the world of today.

At this time there is much active research in the field and little in the way of publicly available implementations. Several approaches have been developed for sensing gestures and controlling robots. Glove based technique is a well-known means of recognizing hand gestures. It utilizes a sensor attached to a glove that directly measures hand movements.
Gestures are useful for computer interaction since they are the most primary and expressive forms of human communication. Gesture interfaces for gaming based on hand/body gesture technology must be designed to achieve social and commercial success. No single method for automatic hand gesture recognition is suitable for every application; each gesture-recognition algorithm depends on user cultural background, application domain, and environment.

<center><img src="https://github.com/SheenaMathew19/minimal/blob/master/gesturerecg.jpg"/></center>
#### Structure of Gesture Technology

In computer interfaces, two types of gestures are distinguished:
##### 1. Offline gestures: 
Those gestures that are processed after the user interaction with the object. An example is the gesture to activate a menu.
#### 2. Online gestures: 
Direct manipulation gestures. They are used to scale or rotate a tangible object.

The ability to track a person’s movements and determine what gestures they may be performing can be achieved through various tools. Although there is a large amount of research done in image/video-based gesture recognition, there is some variation within the tools and environments used between implementations. Some of the tools may include wired gloves, Depth-aware cameras, Stereo cameras, Gesture-based controllers, and also Radar. These controllers act as an extension of the body so that when gestures are performed, some of their motion can be conveniently captured by the software. An example of emerging gesture-based motion capture is skeletal hand tracking, which is being developed for virtual reality and augmented reality applications.

Depending on the type of input data, the approach for interpreting a gesture could be done in different ways. However, most of the techniques rely on key pointers represented in a 3D coordinate system. Based on the relative motion of these, the gesture can be detected with high accuracy, depending on the quality of the input and the algorithm’s approach.
In order to interpret movements of the body, one has to classify them according to common properties and the message the movements may express. For example, in sign language, each gesture represents a word or phrase. The taxonomy that seems very appropriate for Human-Computer Interaction has been proposed by Quek in “Toward a Vision-Based Hand Gesture Interface”. He presents several interactive gesture systems in order to capture the whole space of the gestures: Manipulative, Semaphoric, and Conversational. Some literature differentiates 2 different approaches in gesture recognition: a 3D model-based and an appearance-based. The foremost method makes use of 3D information of key elements of the body parts in order to obtain several important parameters, like palm position or joint angles. On the other hand, Appearance-based systems use images or videos for direct interpretation.

